{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5baaa8ce",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1398,
     "status": "ok",
     "timestamp": 1712811631032,
     "user": {
      "displayName": "Sina",
      "userId": "15595762359228494453"
     },
     "user_tz": 420
    },
    "id": "5baaa8ce",
    "outputId": "9b66886e-0edd-4d9a-a860-ba06e8a3ec16"
   },
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"PUT YOUR KEY HERE\" #This is the place holder for your google cloud tts\n",
    "import ast  # Module to convert string representations of lists into actual lists\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import librosa\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from google.cloud import texttospeech\n",
    "import boto3 #For this one you have to set up your key already in your console"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "365b1b6b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## First Step: Processing the Audio Files\n",
    "\n",
    "The goal of this script is to **automate the process of extracting meaningful audio features** from a collection of `.wav` audio files located within a specified directory. By leveraging the `librosa` library, it accomplishes the following:\n",
    "\n",
    "- **Extracts a set of audio features** for each file, including:\n",
    "  - **Mel-frequency cepstral coefficients (MFCCs):** Capture the timbre of the audio.\n",
    "  - **Zero-crossing rate:** Measures the number of times the signal crosses the horizontal axis.\n",
    "  - **Pitch statistics:** Includes mean and standard deviation of the audio's pitch.\n",
    "  - **Spectral centroid:** Indicates where the center of mass of the spectrum is located.\n",
    "  - **Spectral bandwidth:** Describes the width of the spectral energy distribution.\n",
    "  - **Spectral rolloff:** Measures the shape of the spectrum.\n",
    "  - **Harmonic and percussive components:** Separate the harmonic and percussive elements of the audio signal.\n",
    "\n",
    "These features are **critical in various applications** such as audio classification, music information retrieval, and sound analysis.\n",
    "\n",
    "- **Compiles the extracted features** into a structured format, along with the file and folder names for context, and **saves them into a CSV file**. This streamlined process facilitates easy analysis and modeling by providing a **ready-to-use dataset of audio features**.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d543779",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 433396,
     "status": "ok",
     "timestamp": 1712801338144,
     "user": {
      "displayName": "Sina",
      "userId": "15595762359228494453"
     },
     "user_tz": 420
    },
    "id": "3d543779",
    "outputId": "02fdba18-496c-491d-fcd4-5dd18d4cc63c",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: ccdata/An individual_s culture/citizen_audio_0c4e7b6c-6f28-4efa-bc30-85fefb055b2b.wav\n",
      "Processing file: ccdata/An individual_s culture/citizen_audio_9eb31fe4-a6ab-448e-843a-02424613e681.wav\n",
      "Processing file: ccdata/An individual_s culture/citizen_audio_0a66b438-9651-4748-ae5e-33db97b7b5c0.wav\n",
      "Processing file: ccdata/An individual_s culture/citizen_audio_459d5457-2a8e-4812-bbd3-de9cb15b9094.wav\n",
      "Processing file: ccdata/An individual_s culture/citizen_audio_11ebd1b3-b4b1-4b04-a134-a958feae4cdc.wav\n",
      "Processing file: ccdata/An individual_s culture/citizen_audio_2bd82592-e0b8-426b-8b3c-50d8589708f6.wav\n",
      "Processing file: ccdata/An individual_s culture/citizen_audio_12175079-597a-4854-a039-2cf3ad937bc2.wav\n",
      "Processing file: ccdata/An individual_s culture/citizen_audio_64b6d0cd-237a-481e-888e-6ac308d2b83a.wav\n",
      "Processing file: ccdata/An individual_s culture/citizen_audio_0413c4f8-2eef-4c15-8a68-88db002575a2.wav\n",
      "Processing file: ccdata/An individual_s culture/citizen_audio_630e0f8e-4c1b-4413-b054-cb8a275bc9de.wav\n",
      "Processing file: ccdata/An individual_s culture/citizen_audio_9717d5fb-d902-47a7-baa5-41a81a34da09.wav\n",
      "Processing file: ccdata/An individual_s culture/citizen_audio_b32b482f-5292-4e49-81c6-ed77b2c78758.wav\n",
      "Processing file: ccdata/New knowledge from science/citizen_audio_7955b963-d1f5-47ae-b2ae-0516948bae9c.wav\n",
      "Processing file: ccdata/New knowledge from science/citizen_audio_b7466913-abd4-4b31-9ba6-1ee2de9ab68c.wav\n",
      "Processing file: ccdata/New knowledge from science/citizen_audio_1ed1f565-490a-4fd6-86f5-07f99618dc42.wav\n",
      "Processing file: ccdata/New knowledge from science/citizen_audio_f6a0655d-c1d2-4f2a-b245-37cdbe5c5f02.wav\n",
      "Processing file: ccdata/New knowledge from science/citizen_audio_37c2ca91-8a81-4275-8e18-9ee8390de404.wav\n",
      "Processing file: ccdata/New knowledge from science/citizen_audio_536b3ff7-3eb8-4d10-a1ca-a424b5a33ce9.wav\n",
      "Processing file: ccdata/New knowledge from science/citizen_audio_2dbabeb7-c50e-4f13-87b9-7eb2a3ba8ad2.wav\n",
      "Processing file: ccdata/New knowledge from science/citizen_audio_3210e2da-079e-4cb0-ad0a-35e7eb5e7018.wav\n",
      "Processing file: ccdata/New knowledge from science/citizen_audio_b115ec58-c287-45d3-99a3-4fee147dcd68.wav\n",
      "Processing file: ccdata/New knowledge from science/citizen_audio_c54a4372-416b-459c-9ed1-81b12e26eff6.wav\n",
      "Processing file: ccdata/New knowledge from science/citizen_audio_68f1b8d9-9eb8-4efb-be64-bae4fd9efdc2.wav\n",
      "Processing file: ccdata/New knowledge from science/citizen_audio_ffd5b812-fd2f-4c92-a5aa-30be026ea120.wav\n",
      "Processing file: ccdata/New knowledge from science/citizen_audio_d688e1c7-d06b-4a36-9e99-093cc65b2e82.wav\n",
      "Processing file: ccdata/New knowledge from science/citizen_audio_2ccdb076-a75a-4a37-a73d-f6d0f305fa9a.wav\n",
      "Processing file: ccdata/New knowledge from science/citizen_audio_e1cf8a2e-0c48-4ef5-9cb0-c79a6a81ff74.wav\n",
      "Processing file: ccdata/New knowledge from science/citizen_audio_4762a4d4-37b8-4ffd-9a30-5c207a8d3cae.wav\n",
      "Processing file: ccdata/A house has all kinds of stuff/citizen_audio_91f17e50-84ab-4dcb-8bda-a458f5431e44.wav\n",
      "Processing file: ccdata/A house has all kinds of stuff/citizen_audio_4dd15d7f-de90-4bd5-809a-0c35778c8764.wav\n",
      "Processing file: ccdata/A house has all kinds of stuff/citizen_audio_29927f3e-d6a5-4003-ac9d-4e4595872dff.wav\n",
      "Processing file: ccdata/A house has all kinds of stuff/citizen_audio_468a2ab1-ea39-4c29-a076-cf162918e3b4.wav\n",
      "Processing file: ccdata/A house has all kinds of stuff/citizen_audio_1a49ab7b-783e-48ee-bb5b-a24f8b08fef9.wav\n",
      "Processing file: ccdata/A house has all kinds of stuff/citizen_audio_bbc63bf7-a0f8-4490-b314-1efe71195b53.wav\n",
      "Processing file: ccdata/A house has all kinds of stuff/citizen_audio_92a013e2-1719-4fb5-ab0b-fd270e0ed32c.wav\n",
      "Processing file: ccdata/A house has all kinds of stuff/citizen_audio_390a3c3b-5546-4c65-bd42-34c5c92300e3.wav\n",
      "Processing file: ccdata/A house has all kinds of stuff/citizen_audio_57594ff1-87c1-4027-b8d9-1772c91658a3.wav\n",
      "Processing file: ccdata/A house has all kinds of stuff/citizen_audio_384aa19c-9a39-422c-9e2d-d61d501580c2.wav\n",
      "Processing file: ccdata/A house has all kinds of stuff/citizen_audio_71f65753-d9ce-45ae-898a-89911f90bac1.wav\n",
      "Processing file: ccdata/A house has all kinds of stuff/citizen_audio_d1ada1f5-c779-4fc0-9019-b7a21f620149.wav\n",
      "Processing file: ccdata/A house has all kinds of stuff/citizen_audio_2f9d1fd8-731a-433b-93c9-96c50da398e4.wav\n",
      "Processing file: ccdata/A house has all kinds of stuff/citizen_audio_0ccd4000-40bf-4342-b254-06583b9f55c0.wav\n",
      "Processing file: ccdata/A house has all kinds of stuff/citizen_audio_01d2ccbd-5d5f-47f4-b3e8-0d42bafb2ffc.wav\n",
      "Processing file: ccdata/A house has all kinds of stuff/citizen_audio_5f218b20-3104-4159-b492-7e622fa75c40.wav\n",
      "Processing file: ccdata/A house has all kinds of stuff/citizen_audio_eef9e338-a577-40fc-94aa-23ece2fe0e82.wav\n",
      "Processing file: ccdata/A house has all kinds of stuff/citizen_audio_d6e66193-86d4-4e31-b9a9-9a12d2ab1807.wav\n",
      "Processing file: ccdata/A house has all kinds of stuff/citizen_audio_73f5ede5-e790-4b84-b323-c8c041b14703.wav\n",
      "Processing file: ccdata/A house has all kinds of stuff/citizen_audio_8ce6a097-09ec-45ec-9d6a-f59716040be3.wav\n",
      "Processing file: ccdata/Modern English has been spreading/citizen_audio_7bfa053f-9d21-45d1-be95-2ec47554b81c.wav\n",
      "Processing file: ccdata/Modern English has been spreading/citizen_audio_e3904a30-7517-44b8-b84b-2541aab131d4.wav\n",
      "Processing file: ccdata/Modern English has been spreading/citizen_audio_ad2c4ec7-28f9-4e63-8f42-a91c3c59ddaa.wav\n",
      "Processing file: ccdata/Modern English has been spreading/citizen_audio_12fc9e21-70bd-4fde-87dc-a285b26edea5.wav\n",
      "Processing file: ccdata/Modern English has been spreading/citizen_audio_1c92a5b7-2004-416e-8352-3a9dd55e986a.wav\n",
      "Processing file: ccdata/Modern English has been spreading/citizen_audio_687ee289-00a5-4794-916e-8e8ebcdc7536.wav\n",
      "Processing file: ccdata/Modern English has been spreading/citizen_audio_45daea32-ae56-4f81-a2df-140ccefb61ec.wav\n",
      "Processing file: ccdata/Modern English has been spreading/citizen_audio_8d996ee3-7b83-44e4-8ec9-4e6ac80ea767.wav\n",
      "Processing file: ccdata/Modern English has been spreading/citizen_audio_5e504161-24d7-41d2-9827-95a8000112b1.wav\n",
      "Processing file: ccdata/Modern English has been spreading/citizen_audio_f9b129cb-247a-4dec-84db-1efc6d0e6b0d.wav\n",
      "Processing file: ccdata/Modern English has been spreading/citizen_audio_cfda1447-b820-447e-9fa8-6e956cf6a3ed.wav\n",
      "Processing file: ccdata/Modern English has been spreading/citizen_audio_41d4938d-378a-452f-ba10-70c628c95413.wav\n",
      "Processing file: ccdata/I am an architect/citizen_audio_bcbfbf1f-8bed-422d-9e75-5d38a5f87c36.wav\n",
      "Processing file: ccdata/I am an architect/citizen_audio_301dbc84-1ee4-4196-a153-57d3ae47ecb8.wav\n",
      "Processing file: ccdata/I am an architect/citizen_audio_898004c8-46f5-436a-8e56-ed1cdd2ff9bf.wav\n",
      "Processing file: ccdata/I am an architect/citizen_audio_ef81f6c7-ca99-433b-a61f-c734b2416b32.wav\n",
      "Processing file: ccdata/I am an architect/citizen_audio_f253230c-7438-46f5-ba57-42f7542b566b.wav\n",
      "Processing file: ccdata/I am an architect/citizen_audio_d19a04d0-ce99-4338-b17a-12621d241d02.wav\n",
      "Processing file: ccdata/I am an architect/citizen_audio_4e43bcf2-af44-4f51-bb7b-98cd6c0896f0.wav\n",
      "Processing file: ccdata/I am an architect/citizen_audio_106bc9ad-6f9e-4d5f-9d93-5c02dc6794ab.wav\n",
      "Processing file: ccdata/I am an architect/citizen_audio_a03f1347-8ede-41d6-8313-4bd1aa106a97.wav\n",
      "Processing file: ccdata/I am an architect/citizen_audio_9387781d-81fe-498f-ae1f-fe48817c851f.wav\n",
      "Processing file: ccdata/I am an architect/citizen_audio_5ec677dc-8c12-4ff9-894b-5164e15d485e.wav\n",
      "Processing file: ccdata/I am an architect/citizen_audio_50af456b-b98f-4d0a-adf3-4a25b7c80ade.wav\n",
      "Processing file: ccdata/I am an architect/citizen_audio_2e615a3a-ad06-4c1c-906d-58d4309c84ce.wav\n",
      "Processing file: ccdata/I am an architect/citizen_audio_e115def9-f680-489f-b664-62fcaa7b9a73.wav\n",
      "Processing file: ccdata/I am an architect/citizen_audio_77ff0b88-27e0-4a68-ac16-a2157ea96ca2.wav\n",
      "Processing file: ccdata/I am an architect/citizen_audio_2c6ce7f8-b61f-40e3-ab9d-08aacb0429a0.wav\n",
      "Processing file: ccdata/I am an architect/citizen_audio_0856bd28-1c08-47cb-bd6d-ccce0db3183d.wav\n",
      "Processing file: ccdata/I am an architect/citizen_audio_b179783c-27a8-4d25-a189-b29e5d9e5f5c.wav\n",
      "Processing file: ccdata/I am an architect/citizen_audio_76a99e41-d14d-495b-b828-251c8e869a0b.wav\n",
      "Processing file: ccdata/I am an architect/._citizen_audio_ef81f6c7-ca99-433b-a61f-c734b2416b32.wav\n",
      "Could not process file ccdata/I am an architect/._citizen_audio_ef81f6c7-ca99-433b-a61f-c734b2416b32.wav: \n",
      "Failed to process ccdata/I am an architect/._citizen_audio_ef81f6c7-ca99-433b-a61f-c734b2416b32.wav\n",
      "Processing file: ccdata/I am an architect/citizen_audio_e89fb4a4-2123-42f6-9f12-e7c8b0d47031.wav\n",
      "Processing file: ccdata/To build a house/citizen_audio_b1215418-4645-488d-9308-0f7131d1ffd7.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6040/2455772363.py:3: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  audio, sr = librosa.load(file_path, sr=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: ccdata/To build a house/citizen_audio_710e01c2-2617-414f-8489-c4bf2c88e1f1.wav\n",
      "Processing file: ccdata/To build a house/citizen_audio_865b63c8-a0a7-4846-b6ca-26a85bb15828.wav\n",
      "Processing file: ccdata/To build a house/citizen_audio_98aa52a3-e72f-480f-83d6-e010cff49d24.wav\n",
      "Processing file: ccdata/To build a house/citizen_audio_de324724-de22-4f4f-8121-0d4243e9286b.wav\n",
      "Processing file: ccdata/To build a house/citizen_audio_29ad9fc5-1191-472b-a331-f9c60001c327.wav\n",
      "Processing file: ccdata/To build a house/citizen_audio_bf37ed3e-dcad-4fb4-a67b-cc775e680292.wav\n",
      "Processing file: ccdata/To build a house/citizen_audio_49ef82f0-497a-422c-8533-598d02265ada.wav\n",
      "Processing file: ccdata/To build a house/citizen_audio_8821c0d8-1df8-4afe-a83a-846d0cf208bc.wav\n",
      "Processing file: ccdata/To build a house/citizen_audio_20f28680-b251-4076-a45c-7d3305388a3d.wav\n",
      "Processing file: ccdata/To build a house/citizen_audio_80bdafb6-984b-425e-80ca-f526f10cde57.wav\n",
      "Processing file: ccdata/To build a house/citizen_audio_7f2256c8-9d7e-4c64-bd96-02da65a9f8b6.wav\n",
      "Processing file: ccdata/To build a house/citizen_audio_ad175ef8-0727-448b-b2e5-1eea5929e232.wav\n",
      "Processing file: ccdata/To build a house/citizen_audio_a5e696f9-473c-4c82-97c2-dbcd3c213edd.wav\n",
      "Processing file: ccdata/To build a house/citizen_audio_b8b165c0-416e-45fd-af59-7ad2de7bbd1f.wav\n",
      "Processing file: ccdata/To build a house/citizen_audio_694e7c06-62e2-429b-a11f-9575020baa53.wav\n",
      "Processing file: ccdata/To build a house/citizen_audio_23a2f45a-4627-4f66-a7f1-913c38eb65d7.wav\n",
      "Processing file: ccdata/To build a house/citizen_audio_b52562a0-f1e4-4e56-b411-65c84ab26729.wav\n",
      "Processing file: ccdata/To build a house/citizen_audio_c62cdf4e-f923-4710-ae0d-7374c8573ec2.wav\n",
      "Processing file: ccdata/To build a house/citizen_audio_ed7bb3ff-d4b0-4fa2-ab90-511a170042b1.wav\n",
      "Processing file: ccdata/To build a house/citizen_audio_21b31929-4ad7-4f7f-9ec9-a51a8713bfe7.wav\n",
      "Processing file: ccdata/To build a house/citizen_audio_5a88064e-6ec5-4030-93f8-32b2aa3c0307.wav\n",
      "Processing file: ccdata/To build a house/citizen_audio_b2e171e2-d1f6-4ee2-becf-d054bd3737f8.wav\n",
      "Processing file: ccdata/To build a house/citizen_audio_f35c815e-e505-4353-8417-03f95dbbbbfe.wav\n",
      "Processing file: ccdata/To build a house/citizen_audio_a1383933-ce3b-4fc1-b487-fd6229ece210.wav\n",
      "Processing file: ccdata/Over time, philosophers/citizen_audio_076a4076-92d9-4ca0-8a8e-ac86eff997a7.wav\n",
      "Processing file: ccdata/Over time, philosophers/citizen_audio_caaefabb-af71-41b2-b03b-a56e404d341e.wav\n",
      "Processing file: ccdata/Over time, philosophers/citizen_audio_955959cb-e44f-4016-b030-98dcad1283f9.wav\n",
      "Processing file: ccdata/Over time, philosophers/citizen_audio_5d7450c9-1cd0-4699-a352-76aff041d802.wav\n",
      "Processing file: ccdata/Over time, philosophers/citizen_audio_27ff0444-7fb0-4ee1-b820-3f2771a4ae6b.wav\n",
      "Processing file: ccdata/Over time, philosophers/citizen_audio_854bcca0-5542-4743-8c6e-8588ae973019.wav\n",
      "Processing file: ccdata/Over time, philosophers/citizen_audio_4458e920-67a9-48be-8fcd-7923dbada179.wav\n",
      "Processing file: ccdata/Over time, philosophers/citizen_audio_f71df513-3fb8-4271-88c4-f64cbdacc0e4.wav\n",
      "Processing file: ccdata/Over time, philosophers/citizen_audio_3a0127fc-6a15-46f8-b41a-d52bd7744238.wav\n",
      "Processing file: ccdata/Over time, philosophers/citizen_audio_f8701538-47cf-4928-8b91-78d2fd838a55.wav\n",
      "Processing file: ccdata/Over time, philosophers/citizen_audio_7fd0e835-f662-492c-8903-4ab7d7e99a66.wav\n",
      "Processing file: ccdata/Over time, philosophers/citizen_audio_f037d658-9950-4447-976d-14d2a9b3dc0a.wav\n",
      "Processing file: ccdata/Over time, philosophers/citizen_audio_d3480a4f-dc7e-4688-ab05-1ef5dc7766fd.wav\n",
      "Processing file: ccdata/Over time, philosophers/citizen_audio_39a6c29c-aa18-4096-9ba0-a29b949d6a31.wav\n",
      "Processing file: ccdata/Over time, philosophers/citizen_audio_17a65e83-5015-4713-addc-53fd46c72849.wav\n",
      "Processing file: ccdata/Over time, philosophers/citizen_audio_41acd23f-2c0b-4313-b796-57ea737466a6.wav\n",
      "Processing file: ccdata/Over time, philosophers/citizen_audio_8c7c2295-fa5f-4a8e-ae49-1b4583ed5261.wav\n",
      "Processing file: ccdata/Mathematics is essential/citizen_audio_c2f7d769-414d-44c7-8ae2-d91fc8a24228.wav\n",
      "Processing file: ccdata/Mathematics is essential/citizen_audio_005ca4d0-b2d1-4728-be56-86c72450c20c.wav\n",
      "Processing file: ccdata/Mathematics is essential/citizen_audio_d7640076-8d8b-4e0b-a292-15f4fd5ff8ed.wav\n",
      "Processing file: ccdata/Mathematics is essential/citizen_audio_35ca0bf3-f531-461c-8b2a-8075458c6e1c.wav\n",
      "Processing file: ccdata/Mathematics is essential/citizen_audio_d3c67432-b375-424a-8ed2-611b038632bf.wav\n",
      "Processing file: ccdata/Mathematics is essential/citizen_audio_c662ebe4-bbb9-4fa2-aa7c-5e8d69bbc50b.wav\n",
      "Processing file: ccdata/Mathematics is essential/citizen_audio_1b8b52e0-7b13-4c77-aea5-f7a66a6a5512.wav\n",
      "Processing file: ccdata/Mathematics is essential/citizen_audio_a3162132-515d-4a85-af81-fc650d07f49c.wav\n",
      "Processing file: ccdata/Mathematics is essential/citizen_audio_0f18bf81-2402-47df-998f-1d44e590d537.wav\n",
      "Processing file: ccdata/Mathematics is essential/citizen_audio_b02c17b6-3c51-409f-8226-71688e48cac9.wav\n",
      "Processing file: ccdata/Mathematics is essential/citizen_audio_f1cd4e7e-781f-4e95-8669-f367656b6e4f.wav\n",
      "Processing file: ccdata/Mathematics is essential/citizen_audio_be30cf18-90b8-4910-82a9-e41f4d077e73.wav\n",
      "Processing file: ccdata/Mathematics is essential/citizen_audio_de281e67-6b43-4266-9289-cb7ee7257c32.wav\n",
      "Processing file: ccdata/Mathematics is essential/citizen_audio_2662e5e9-eb23-46c5-b90a-57d137b4ed73.wav\n",
      "Processing file: ccdata/Mathematics is essential/citizen_audio_51ab3e8d-f4fb-4525-9c50-b2fa4497fd7f.wav\n",
      "Processing file: ccdata/Mathematics is essential/citizen_audio_619bbc13-911d-48e4-a091-4bc5ad101f34.wav\n",
      "Processing file: ccdata/Mathematics is essential/citizen_audio_b7b09af1-25be-4f4d-96ee-59deeeab129a.wav\n",
      "Processing file: ccdata/Mathematics is essential/citizen_audio_f288ffaf-076e-4948-8467-f49d443fa6c0.wav\n",
      "Processing file: ccdata/Mathematics is essential/citizen_audio_8c4cd5fd-d8d4-4cf0-82e9-4c2ae483a794.wav\n",
      "Processing file: ccdata/Mathematics is essential/citizen_audio_71ab7ec2-e40c-4415-a7c5-21f82536f931.wav\n",
      "Processing file: ccdata/Peg and Datiz/citizen_audio_97a08d75-267b-4037-9268-ffaf058970c9.wav\n",
      "Processing file: ccdata/Peg and Datiz/citizen_audio_57bec0f5-ad3c-43b0-afe1-444e6d608ebb.wav\n",
      "Processing file: ccdata/Peg and Datiz/citizen_audio_2454a642-45f2-416e-8827-6f8abaa8d08c.wav\n",
      "Processing file: ccdata/Peg and Datiz/citizen_audio_a8ea29dd-1b4c-493f-87a4-4957290e9469.wav\n",
      "Processing file: ccdata/Peg and Datiz/citizen_audio_51169265-10ce-441a-81ca-479a904125f5.wav\n",
      "Processing file: ccdata/Peg and Datiz/citizen_audio_2b6b9dec-b46b-49bb-bf49-c5b1fd7426e9.wav\n",
      "Processing file: ccdata/Peg and Datiz/citizen_audio_d700a12a-dbd6-489f-99b8-acca1bed290c.wav\n",
      "Processing file: ccdata/Peg and Datiz/citizen_audio_bc7bb375-59e3-48c1-aa45-7e31f5fb4910.wav\n",
      "Processing file: ccdata/Peg and Datiz/citizen_audio_fc678202-d0e5-4789-a0b8-289515b0493b.wav\n",
      "Processing file: ccdata/Peg and Datiz/citizen_audio_c66cad30-03f3-4a2d-9b69-81c359107c61.wav\n",
      "Processing file: ccdata/Peg and Datiz/citizen_audio_9cdbac22-3c8d-4456-a548-dcb12b22f24b.wav\n",
      "Processing file: ccdata/Peg and Datiz/citizen_audio_98f4dfd1-eeba-4428-a61b-ddcc667cd8c6.wav\n",
      "Processing file: ccdata/Peg and Datiz/citizen_audio_56308d42-5fc5-4d2d-90f7-61898c8c6bde.wav\n",
      "Processing file: ccdata/Peg and Datiz/citizen_audio_fde04bfb-d304-4523-a00d-32672453f280.wav\n",
      "Processing file: ccdata/Peg and Datiz/citizen_audio_28835598-3d14-416a-ade4-c53c0c5c2e20.wav\n",
      "Processing file: ccdata/Peg and Datiz/citizen_audio_06cb76a2-2814-40de-966e-f740c7165618.wav\n",
      "Processing file: ccdata/Peg and Datiz/citizen_audio_77813f57-da1d-4f34-8115-c3d426f52249.wav\n",
      "Processing file: ccdata/Peg and Datiz/citizen_audio_4e890b1e-c7d7-46e3-8250-6afbbe79ac5e.wav\n",
      "Feature extraction complete. Results saved to enhanced_audio_features.csv\n"
     ]
    }
   ],
   "source": [
    "def extract_features(file_path):\n",
    "    try:\n",
    "        audio, sr = librosa.load(file_path, sr=None)\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)\n",
    "        zcr = librosa.feature.zero_crossing_rate(y=audio)\n",
    "        pitch, _ = librosa.piptrack(y=audio, sr=sr)\n",
    "        spectral_centroid = librosa.feature.spectral_centroid(y=audio, sr=sr)\n",
    "        spectral_bandwidth = librosa.feature.spectral_bandwidth(y=audio, sr=sr)\n",
    "        spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=sr)\n",
    "        harmonic, percussive = librosa.effects.hpss(audio)\n",
    "\n",
    "        features = {\n",
    "            'MFCCs': np.mean(mfccs, axis=1).tolist(),\n",
    "            'ZCR': np.mean(zcr).item(),\n",
    "            'PitchMean': np.mean(pitch).item(),\n",
    "            'PitchStd': np.std(pitch).item(),\n",
    "            'SpectralCentroid': np.mean(spectral_centroid).item(),\n",
    "            'SpectralBandwidth': np.mean(spectral_bandwidth).item(),\n",
    "            'SpectralRolloff': np.mean(spectral_rolloff).item(),\n",
    "            'Harmonic': np.mean(harmonic).item(),\n",
    "            'Percussive': np.mean(percussive).item(),\n",
    "        }\n",
    "        return features\n",
    "    except Exception as e:\n",
    "        print(f\"Could not process file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_audio_directory(directory_path):\n",
    "    audio_features = []\n",
    "    for root, dirs, files in os.walk(directory_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.wav'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                folder_name = os.path.basename(root)\n",
    "                print(f\"Processing file: {file_path}\")\n",
    "                features = extract_features(file_path)\n",
    "                if features:\n",
    "                    features['Folder'] = folder_name  # Add folder name\n",
    "                    features['File'] = file  # Add file name\n",
    "                    audio_features.append(features)\n",
    "                else:\n",
    "                    print(f\"Failed to process {file_path}\")\n",
    "    return audio_features\n",
    "\n",
    "ccdata_path = 'ccdata'\n",
    "audio_features = process_audio_directory(ccdata_path)\n",
    "\n",
    "# Ensure audio_features is not empty before proceeding\n",
    "if audio_features:\n",
    "    # Define column order with 'Folder' and 'File' at the beginning\n",
    "    column_order = ['Folder', 'File'] + [col for col in audio_features[0] if col not in ['Folder', 'File']]\n",
    "    # Convert to DataFrame with specified column order\n",
    "    df_audio_features = pd.DataFrame(audio_features, columns=column_order)\n",
    "    output_file = 'enhanced_audio_features.csv'  # Full path to save in your Drive\n",
    "    df_audio_features.to_csv(output_file, index=False)\n",
    "    print(f\"Feature extraction complete. Results saved to {output_file}\")\n",
    "else:\n",
    "    print(\"No audio features were extracted. Please check the files and their format.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf32b4d",
   "metadata": {
    "id": "9cf32b4d"
   },
   "source": [
    "---\n",
    "\n",
    "## Second Step: Classification\n",
    "After successfully completing the initial phase of audio feature extraction and securing the \"enhanced_audio_features.csv\", a crucial enhancement was introduced to the dataset. I personally augmented the data with five pivotal columns: \"Clarity\", \"Confidence\", \"Agreeableness\", \"Pace\", and \"Volume Dynamics\", alongside a binary \"Compelling (0 or 1)\" indicator. This manual enrichment was driven by a comprehensive auditory analysis, wherein I meticulously listened to each voice recording, discerning and annotating these nuanced traits. This labor-intensive process was not merely for data augmentation but aimed at crafting a robust foundation for our classification endeavor.\n",
    "\n",
    "With this enriched dataset, dubbed \"final_classification_features.csv\", the journey towards developing a predictive model commenced. The preparatory steps involved ensuring the dataset's integrity, which included verifying the presence and accurate spelling of each column, especially the critical \"Compelling\" column which serves as our target variable. The non-essential \"Folder\" and \"File\" columns, primarily used for tracking and organization, were excluded from the model training dataset to maintain focus on the features directly influencing voice compellingness.\n",
    "\n",
    "Transforming the \"MFCCs\" column from a string representation to a list and subsequently expanding it into individual features was a pivotal preprocessing step. This transformation facilitated a detailed feature analysis, allowing each Mel-frequency cepstral coefficient to contribute independently to the model's learning process.\n",
    "\n",
    "The classification model's development phase encompassed the standard practices of splitting the dataset into training and testing subsets, feature scaling for normalization, and finally, model training and evaluation. A Gradient Boosting Classifier, known for its effectiveness in handling diverse datasets, was chosen as the predictive model. This model was then rigorously trained on the scaled training data.\n",
    "\n",
    "Evaluation metrics and a confusion matrix provided insights into the model's performance, revealing its strengths and areas for improvement. Additionally, an analysis of feature importance was conducted, offering a window into the attributes most influential in determining the compellingness of a voice. This not only informed the model refinement process but also deepened the understanding of what makes a voice compelling from an auditory perspective.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "SHn2GJHY3cGK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 406,
     "status": "ok",
     "timestamp": 1712802405364,
     "user": {
      "displayName": "Sina",
      "userId": "15595762359228494453"
     },
     "user_tz": 420
    },
    "id": "SHn2GJHY3cGK",
    "outputId": "87488ce3-d198-4998-a64e-aa32d140b50b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Folder', 'File', 'MFCCs', 'ZCR', 'PitchMean', 'PitchStd',\n",
      "       'SpectralCentroid', 'SpectralBandwidth', 'SpectralRolloff', 'Harmonic',\n",
      "       'Percussive', 'Clarity', 'Confidence', 'Agreeableness', 'Pace',\n",
      "       'Volume Dynamics', 'Compelling'],\n",
      "      dtype='object')\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.95      0.82        19\n",
      "           1       0.86      0.46      0.60        13\n",
      "\n",
      "    accuracy                           0.75        32\n",
      "   macro avg       0.79      0.70      0.71        32\n",
      "weighted avg       0.78      0.75      0.73        32\n",
      "\n",
      "Confusion Matrix:\n",
      " [[18  1]\n",
      " [ 7  6]]\n",
      "Feature Importances:\n",
      "                      importance\n",
      "Agreeableness      7.240002e-01\n",
      "Volume Dynamics    4.050539e-02\n",
      "MFCC_12            3.072972e-02\n",
      "MFCC_8             2.424984e-02\n",
      "Clarity            2.371334e-02\n",
      "SpectralRolloff    2.268341e-02\n",
      "MFCC_10            1.907508e-02\n",
      "MFCC_9             1.628832e-02\n",
      "SpectralCentroid   1.231467e-02\n",
      "MFCC_7             1.185456e-02\n",
      "Confidence         1.125328e-02\n",
      "MFCC_4             9.883281e-03\n",
      "MFCC_6             8.983906e-03\n",
      "ZCR                8.954730e-03\n",
      "SpectralBandwidth  8.713141e-03\n",
      "MFCC_1             8.449898e-03\n",
      "PitchStd           3.839629e-03\n",
      "Pace               3.820129e-03\n",
      "PitchMean          3.264137e-03\n",
      "Percussive         2.630615e-03\n",
      "MFCC_5             1.761945e-03\n",
      "MFCC_0             1.498683e-03\n",
      "Harmonic           1.166666e-03\n",
      "MFCC_11            3.391121e-04\n",
      "MFCC_2             2.629106e-05\n",
      "MFCC_3             5.997918e-08\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('final_classificaiton_features.csv')\n",
    "# Print all column names to see if 'Compelling' is present and correctly spelled\n",
    "print(df.columns)\n",
    "# Assuming 'Folder' and 'File' columns are not needed for model training\n",
    "df.drop(['Folder', 'File'], axis=1, inplace=True)\n",
    "\n",
    "# Convert string representations in 'MFCCs' to lists and expand them into separate columns\n",
    "df['MFCCs'] = df['MFCCs'].apply(ast.literal_eval)\n",
    "mfccs_df = pd.DataFrame(df['MFCCs'].tolist(), index=df.index)\n",
    "mfccs_df.columns = ['MFCC_' + str(i) for i in range(len(mfccs_df.columns))]\n",
    "df = pd.concat([df.drop(['MFCCs'], axis=1), mfccs_df], axis=1)\n",
    "\n",
    "# Define features and target\n",
    "X = df.drop(['Compelling'], axis=1)\n",
    "y = df['Compelling']\n",
    "\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Feature scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Model training\n",
    "model = GradientBoostingClassifier(random_state=42)\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Model evaluation\n",
    "y_pred = model.predict(X_test_scaled)\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Feature importance\n",
    "feature_importances = pd.DataFrame(model.feature_importances_, index=X_train.columns, columns=['importance']).sort_values('importance', ascending=False)\n",
    "print(\"Feature Importances:\\n\", feature_importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QFwamug45e2w",
   "metadata": {
    "id": "QFwamug45e2w"
   },
   "source": [
    "---\n",
    "\n",
    "## Enhancing the Project: Introducing Text-to-Speech Synthesis\n",
    "\n",
    "Building upon the insights gained from the classification model, the next step in our exploration involves leveraging Google Cloud's Text-to-Speech (TTS) API to synthesize speech from text. This process not only enriches our dataset but also provides a practical avenue to assess the accuracy and applicability of our classification results in real-world scenarios.\n",
    "\n",
    "### Objective\n",
    "\n",
    "The objective here is to dynamically generate audio files using various voice configurations to simulate different speaking styles and accents. This allows us to:\n",
    "\n",
    "- Evaluate the robustness of our classification model across a diverse range of voices.\n",
    "- Understand how different voice characteristics (e.g., pitch, speaking rate) might influence audience engagement, as predicted by our model.\n",
    "\n",
    "### Implementation\n",
    "\n",
    "1. **Initialize the Text-to-Speech Client:** A crucial first step to access Google Cloud's TTS service, enabling us to convert text into natural-sounding speech.\n",
    "\n",
    "2. **Define the Text:** We select a piece of text intended for an engaging presentation, serving as the base content for our speech synthesis.\n",
    "\n",
    "3. **Configure Voices:** A series of voice configurations are defined to cover a broad spectrum of languages, accents, genders, pitches, and speaking rates. These variations aim to mimic real-world speaking conditions as closely as possible.\n",
    "\n",
    "4. **Synthesize Speech:** For each voice configuration, we:\n",
    "   - Convert the specified text into speech.\n",
    "   - Adjust the voice parameters according to our predefined configurations.\n",
    "   - Generate an MP3 file for each configuration, allowing for a practical and accessible way to review the synthesized speech.\n",
    "\n",
    "By integrating text-to-speech synthesis into our project, we bridge the gap between theoretical data analysis and tangible, audible results. This step not only enhances our dataset but also lays the groundwork for a deeper understanding of how voice attributes impact listener perception and engagement.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "220defca",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2108,
     "status": "ok",
     "timestamp": 1712799254919,
     "user": {
      "displayName": "Sina",
      "userId": "15595762359228494453"
     },
     "user_tz": 420
    },
    "id": "220defca",
    "outputId": "21d35a9f-626c-4b5a-81ab-efdb8aebe85a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Audio content written to file \"output_voice_1.mp3\"\n",
      "Audio content written to file \"output_voice_2.mp3\"\n",
      "Audio content written to file \"output_voice_3.mp3\"\n",
      "Audio content written to file \"output_voice_4.mp3\"\n",
      "Audio content written to file \"output_voice_5.mp3\"\n",
      "Audio content written to file \"output_voice_6.mp3\"\n",
      "Audio content written to file \"output_voice_7.mp3\"\n",
      "Audio content written to file \"output_voice_8.mp3\"\n",
      "Audio content written to file \"output_voice_9.mp3\"\n",
      "Audio content written to file \"output_voice_10.mp3\"\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Text-to-Speech client\n",
    "client = texttospeech.TextToSpeechClient()\n",
    "\n",
    "# Define the text to be spoken\n",
    "text = \"Hi, today I'm going to teach you guys how to make an engaging presentation. In order to get people engaged, we need to make eye contact with our audience.\"\n",
    "\n",
    "# Define a list of configurations for different voices\n",
    "voice_configs = [\n",
    "    {\"language_code\": \"en-US\", \"name\": \"en-US-Wavenet-A\", \"ssml_gender\": texttospeech.SsmlVoiceGender.MALE, \"pitch\": 0, \"speaking_rate\": 1.0},\n",
    "    {\"language_code\": \"en-US\", \"name\": \"en-US-Wavenet-D\", \"ssml_gender\": texttospeech.SsmlVoiceGender.MALE, \"pitch\": 5, \"speaking_rate\": 1.2},\n",
    "    {\"language_code\": \"en-GB\", \"name\": \"en-GB-Wavenet-A\", \"ssml_gender\": texttospeech.SsmlVoiceGender.FEMALE, \"pitch\": -5, \"speaking_rate\": 0.9},\n",
    "    {\"language_code\": \"en-AU\", \"name\": \"en-AU-Wavenet-C\", \"ssml_gender\": texttospeech.SsmlVoiceGender.FEMALE, \"pitch\": 2, \"speaking_rate\": 1.1},\n",
    "    {\"language_code\": \"en-IN\", \"name\": \"en-IN-Wavenet-A\", \"ssml_gender\": texttospeech.SsmlVoiceGender.MALE, \"pitch\": -2, \"speaking_rate\": 0.95},\n",
    "    {\"language_code\": \"en-US\", \"name\": \"en-US-Wavenet-E\", \"ssml_gender\": texttospeech.SsmlVoiceGender.FEMALE, \"pitch\": 10, \"speaking_rate\": 0.8},\n",
    "    {\"language_code\": \"en-US\", \"name\": \"en-US-Wavenet-F\", \"ssml_gender\": texttospeech.SsmlVoiceGender.NEUTRAL, \"pitch\": -10, \"speaking_rate\": 1.3},\n",
    "    {\"language_code\": \"en-GB\", \"name\": \"en-GB-Wavenet-D\", \"ssml_gender\": texttospeech.SsmlVoiceGender.MALE, \"pitch\": 3, \"speaking_rate\": 1.05},\n",
    "    {\"language_code\": \"en-AU\", \"name\": \"en-AU-Wavenet-A\", \"ssml_gender\": texttospeech.SsmlVoiceGender.MALE, \"pitch\": -3, \"speaking_rate\": 1.15},\n",
    "    {\"language_code\": \"en-US\", \"name\": \"en-US-Wavenet-B\", \"ssml_gender\": texttospeech.SsmlVoiceGender.FEMALE, \"pitch\": 6, \"speaking_rate\": 0.85},\n",
    "]\n",
    "\n",
    "# Iterate over the voice configurations and synthesize speech\n",
    "for i, config in enumerate(voice_configs, start=1):\n",
    "    synthesis_input = texttospeech.SynthesisInput(text=text)\n",
    "    voice_params = texttospeech.VoiceSelectionParams(\n",
    "        language_code=config[\"language_code\"],\n",
    "        name=config[\"name\"],\n",
    "        ssml_gender=config[\"ssml_gender\"]\n",
    "    )\n",
    "    audio_config = texttospeech.AudioConfig(\n",
    "        audio_encoding=texttospeech.AudioEncoding.MP3,\n",
    "        pitch=config[\"pitch\"],\n",
    "        speaking_rate=config[\"speaking_rate\"]\n",
    "    )\n",
    "\n",
    "    # Perform the text-to-speech request\n",
    "    response = client.synthesize_speech(input=synthesis_input, voice=voice_params, audio_config=audio_config)\n",
    "\n",
    "    # Save the output to an MP3 file\n",
    "    filename = f'output_voice_{i}.mp3'  # Adjust path as needed\n",
    "    with open(filename, 'wb') as out:\n",
    "        out.write(response.audio_content)\n",
    "        print(f'Audio content written to file \"{filename}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c4146d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Expanding the Project: Leveraging Amazon Polly for Text-to-Speech\n",
    "\n",
    "In this phase, we utilize Amazon Polly, a service that turns text into lifelike speech, to further enhance our project. Amazon Polly allows us to explore an extensive range of voices and languages, providing a rich set of options for speech synthesis.\n",
    "\n",
    "### Objective\n",
    "\n",
    "The goal is to experiment with different voices and configurations, including the neural engine for more natural-sounding speech. This exploration includes:\n",
    "\n",
    "- Verifying which voices support the neural text-to-speech (TTS) engine for superior audio quality.\n",
    "- Demonstrating various speaking styles, rates, and volumes to simulate real-life scenarios and presentations.\n",
    "\n",
    "### Implementation\n",
    "\n",
    "1. **Initialize Amazon Polly Client:** Set up the client using AWS SDK for Python (Boto3), enabling access to the Polly service.\n",
    "\n",
    "2. **Check Neural Engine Support:** Before synthesizing speech, we ensure the selected voice supports the neural engine, promising a more natural and lifelike audio output.\n",
    "\n",
    "3. **Define Text and Voice Configurations:** A predefined text is ready to be synthesized, and a set of voice configurations is prepared, including one that demonstrates the 'newscaster' speaking style with the voice 'Matthew'.\n",
    "\n",
    "4. **Synthesize and Save Speech:** For each configuration, we:\n",
    "   - Create SSML (Speech Synthesis Markup Language) text to include specific speaking styles if required.\n",
    "   - Call Amazon Polly's `synthesize_speech` method to generate speech.\n",
    "   - Save the synthesized speech to an MP3 file for each voice configuration.\n",
    "\n",
    "Through integrating Amazon Polly, we can produce a diverse array of speech samples from text, enhancing our project's ability to model and analyze speech patterns. This step is not just about expanding our dataset but also about understanding the impact of speech synthesis technologies on communication and presentation skills.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "AHjpkVeBbVqd",
   "metadata": {
    "id": "AHjpkVeBbVqd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Amazon Polly audio content written to file \"polly_neural_voice_1.mp3\"\n",
      "Amazon Polly audio content written to file \"polly_neural_voice_2.mp3\"\n",
      "Amazon Polly audio content written to file \"polly_neural_voice_3.mp3\"\n",
      "Amazon Polly audio content written to file \"polly_neural_voice_4.mp3\"\n",
      "Amazon Polly audio content written to file \"polly_neural_voice_5.mp3\"\n"
     ]
    }
   ],
   "source": [
    "polly_client = boto3.client('polly')\n",
    "\n",
    "def voice_supports_neural_engine(voice_id):\n",
    "    response = polly_client.describe_voices(Engine='neural')\n",
    "    neural_voices = [voice['Id'] for voice in response['Voices']]\n",
    "    return voice_id in neural_voices\n",
    "\n",
    "text = \"Hi, today I'm going to teach you guys how to make an engaging presentation. To get people engaged, we need to make eye contact with our audience.\"\n",
    "\n",
    "# Include a demonstration of the newscaster speaking style for 'Matthew' as an example\n",
    "voice_configs = [\n",
    "    {\"voice_id\": \"Joanna\", \"speaking_rate\": \"1.0\", \"volume\": \"medium\"},\n",
    "    {\"voice_id\": \"Matthew\", \"speaking_rate\": \"1.2\", \"volume\": \"loud\", \"style\": \"newscaster\"},\n",
    "    {\"voice_id\": \"Salli\", \"speaking_rate\": \"0.9\", \"volume\": \"soft\"},\n",
    "    {\"voice_id\": \"Kimberly\", \"speaking_rate\": \"1.05\", \"volume\": \"medium\"},\n",
    "    {\"voice_id\": \"Joey\", \"speaking_rate\": \"1.1\", \"volume\": \"loud\"}\n",
    "]\n",
    "\n",
    "for i, config in enumerate(voice_configs, start=1):\n",
    "    if not voice_supports_neural_engine(config[\"voice_id\"]):\n",
    "        print(f\"Voice {config['voice_id']} does not support Neural engine. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # Adjust SSML to include the newscaster speaking style if specified\n",
    "    if 'style' in config and config['style'] == 'newscaster':\n",
    "        ssml_text = f\"\"\"\n",
    "        <speak>\n",
    "            <amazon:domain name=\"news\">\n",
    "                {text}\n",
    "            </amazon:domain>\n",
    "        </speak>\n",
    "        \"\"\".strip()\n",
    "    else:\n",
    "        ssml_text = f\"\"\"\n",
    "        <speak>\n",
    "            <prosody rate='{config['speaking_rate']}' volume='{config['volume']}'>\n",
    "                {text}\n",
    "            </prosody>\n",
    "            <break time=\"500ms\"/>\n",
    "            Remember, the key to a great presentation is not just what you say, but how you say it.\n",
    "        </speak>\n",
    "        \"\"\".strip()\n",
    "\n",
    "    try:\n",
    "        response = polly_client.synthesize_speech(\n",
    "            Engine='neural',\n",
    "            Text=ssml_text,\n",
    "            TextType='ssml',\n",
    "            OutputFormat='mp3',\n",
    "            VoiceId=config[\"voice_id\"]\n",
    "        )\n",
    "\n",
    "        filename = f'polly_neural_voice_{i}.mp3'\n",
    "        with open(filename, 'wb') as file:\n",
    "            file.write(response['AudioStream'].read())\n",
    "            print(f'Amazon Polly audio content written to file \"{filename}\"')\n",
    "    except boto3.exceptions.Boto3Error as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fe31a1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Audio Processing for the Generated Voices\n",
    "#### Here I did the same processing for the audio files but this time for the generated ones and got them in a dataset\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4019606-1994-4c7d-af7d-7cec97d03e5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: audio_folder\\output_voice_1.mp3\n",
      "Processing file: audio_folder\\output_voice_10.mp3\n",
      "Processing file: audio_folder\\output_voice_2.mp3\n",
      "Processing file: audio_folder\\output_voice_3.mp3\n",
      "Processing file: audio_folder\\output_voice_4.mp3\n",
      "Processing file: audio_folder\\output_voice_5.mp3\n",
      "Processing file: audio_folder\\output_voice_6.mp3\n",
      "Processing file: audio_folder\\output_voice_7.mp3\n",
      "Processing file: audio_folder\\output_voice_8.mp3\n",
      "Processing file: audio_folder\\output_voice_9.mp3\n",
      "Processing file: audio_folder\\polly_neural_voice_1.mp3\n",
      "Processing file: audio_folder\\polly_neural_voice_2.mp3\n",
      "Processing file: audio_folder\\polly_neural_voice_3.mp3\n",
      "Processing file: audio_folder\\polly_neural_voice_4.mp3\n",
      "Processing file: audio_folder\\polly_neural_voice_5.mp3\n",
      "Feature extraction complete. Results saved to enhanced_audio_features_for_generated_voices.csv\n"
     ]
    }
   ],
   "source": [
    "def extract_features(file_path):\n",
    "    try:\n",
    "        audio, sr = librosa.load(file_path, sr=None)\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sr, n_mfcc=13)\n",
    "        zcr = librosa.feature.zero_crossing_rate(y=audio)\n",
    "        pitch, _ = librosa.piptrack(y=audio, sr=sr)\n",
    "        spectral_centroid = librosa.feature.spectral_centroid(y=audio, sr=sr)\n",
    "        spectral_bandwidth = librosa.feature.spectral_bandwidth(y=audio, sr=sr)\n",
    "        spectral_rolloff = librosa.feature.spectral_rolloff(y=audio, sr=sr)\n",
    "        harmonic, percussive = librosa.effects.hpss(audio)\n",
    "\n",
    "        features = {\n",
    "            'MFCCs': np.mean(mfccs, axis=1).tolist(),\n",
    "            'ZCR': np.mean(zcr).item(),\n",
    "            'PitchMean': np.mean(pitch).item(),\n",
    "            'PitchStd': np.std(pitch).item(),\n",
    "            'SpectralCentroid': np.mean(spectral_centroid).item(),\n",
    "            'SpectralBandwidth': np.mean(spectral_bandwidth).item(),\n",
    "            'SpectralRolloff': np.mean(spectral_rolloff).item(),\n",
    "            'Harmonic': np.mean(harmonic).item(),\n",
    "            'Percussive': np.mean(percussive).item(),\n",
    "        }\n",
    "        return features\n",
    "    except Exception as e:\n",
    "        print(f\"Could not process file {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_audio_directory(directory_path):\n",
    "    audio_features = []\n",
    "    for root, dirs, files in os.walk(directory_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.mp3'):  # Changed from .wav to .mp3\n",
    "                file_path = os.path.join(root, file)\n",
    "                folder_name = os.path.basename(root)\n",
    "                print(f\"Processing file: {file_path}\")\n",
    "                features = extract_features(file_path)\n",
    "                if features:\n",
    "                    features['Folder'] = folder_name  # Add folder name\n",
    "                    features['File'] = file  # Add file name\n",
    "                    audio_features.append(features)\n",
    "                else:\n",
    "                    print(f\"Failed to process {file_path}\")\n",
    "    return audio_features\n",
    "\n",
    "# Adjust this path to where your mp3 files are located\n",
    "audio_folder_path = 'audio_folder'\n",
    "audio_features = process_audio_directory(audio_folder_path)\n",
    "\n",
    "# Ensure audio_features is not empty before proceeding\n",
    "if audio_features:\n",
    "    # Define column order with 'Folder' and 'File' at the beginning\n",
    "    column_order = ['Folder', 'File'] + [col for col in audio_features[0] if col not in ['Folder', 'File']]\n",
    "    # Convert to DataFrame with specified column order\n",
    "    df_audio_features = pd.DataFrame(audio_features, columns=column_order)\n",
    "    output_file = 'enhanced_audio_features_for_generated_voices.csv'\n",
    "    df_audio_features.to_csv(output_file, index=False)\n",
    "    print(f\"Feature extraction complete. Results saved to {output_file}\")\n",
    "else:\n",
    "    print(\"No audio features were extracted. Please check the files and their format.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffef482c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Audio Processing for the Generated Voices\n",
    "\n",
    "### Objective\n",
    "\n",
    "I decided to get all the averages for all the columns in our \"final_classificaiton_features.csv\" exluding the hand labeled columns and compare them to each row(audio files) of the new generated voices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d8e152d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_mfccs(df):\n",
    "    # Convert 'MFCCs' column from string representation of lists to actual lists of floats\n",
    "    df['MFCCs'] = df['MFCCs'].apply(ast.literal_eval)\n",
    "\n",
    "    # Assuming each list in 'MFCCs' has a uniform length (e.g., 13 MFCCs)\n",
    "    # Expand each list into its own set of columns 'MFCC_0', 'MFCC_1', ..., 'MFCC_12'\n",
    "    mfcc_columns = ['MFCC_' + str(i) for i in range(len(df['MFCCs'].iloc[0]))]\n",
    "    df[mfcc_columns] = pd.DataFrame(df['MFCCs'].tolist(), index=df.index)\n",
    "\n",
    "    return df\n",
    "\n",
    "# Load the datasets\n",
    "df_classification = pd.read_csv('final_classificaiton_features.csv')\n",
    "df_generated_voices = pd.read_csv('enhanced_audio_features_for_generated_voices.csv')\n",
    "\n",
    "# Preprocess 'MFCCs' for both datasets\n",
    "df_classification = preprocess_mfccs(df_classification)\n",
    "df_generated_voices = preprocess_mfccs(df_generated_voices)\n",
    "\n",
    "# Create a DataFrame with just the averages of the compelling voice features, \n",
    "# repeated to match the number of rows in the generated voices dataset\n",
    "compelling_averages_df = pd.DataFrame([avg_compelling_features] * len(df_generated_voices), \n",
    "                                      columns=avg_compelling_features.index).reset_index(drop=True)\n",
    "\n",
    "# Include 'File' column from df_generated_voices in the reset index step\n",
    "df_generated_voices_reset = df_generated_voices[['File'] + feature_columns].reset_index(drop=True)\n",
    "\n",
    "# Concatenate the two DataFrames side by side for direct comparison\n",
    "# Now including 'File' column as part of the generated voices' features\n",
    "comparison_df = pd.concat([df_generated_voices_reset.add_suffix('_Generated'), \n",
    "                           compelling_averages_df.add_suffix('_CompellingAvg')], axis=1)\n",
    "\n",
    "# Note: The '_Generated' suffix will also be applied to the 'File' column, \n",
    "# resulting in 'File_Generated' to distinguish it clearly\n",
    "\n",
    "# Save the final DataFrame to a CSV file for external analysis or review\n",
    "comparison_df.to_csv('comparison_generated_vs_compelling_with_files.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ca6092",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Audio Processing for the Generated Voices cont.\n",
    "\n",
    "### Objective\n",
    "\n",
    "After getting the new dataset with all the rows with the new generated voices data and new columns with all the averages for \n",
    "all the compelling(1) voices; I decided to find euclidian diistance and find which voices are the closes to the avg of compelling voices data,\n",
    "and compare them to our survey result\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "20ab02ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              File_Generated  Distance_to_Avg_Compelling\n",
      "12  polly_neural_voice_3.mp3                    7.577815\n",
      "10  polly_neural_voice_1.mp3                    9.117620\n",
      "6         output_voice_6.mp3                   10.580027\n",
      "3         output_voice_3.mp3                   11.309343\n",
      "4         output_voice_4.mp3                   11.760292\n",
      "9         output_voice_9.mp3                   11.867753\n",
      "7         output_voice_7.mp3                   11.932012\n",
      "13  polly_neural_voice_4.mp3                   12.212530\n",
      "11  polly_neural_voice_2.mp3                   12.249074\n",
      "2         output_voice_2.mp3                   12.462378\n",
      "1        output_voice_10.mp3                   12.598452\n",
      "0         output_voice_1.mp3                   12.599167\n",
      "8         output_voice_8.mp3                   13.407799\n",
      "5         output_voice_5.mp3                   13.692681\n",
      "14  polly_neural_voice_5.mp3                   14.634078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sinoo\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:464: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Select the feature columns for the generated voices and compelling averages\n",
    "features_generated = [f'MFCC_{i}_Generated' for i in range(13)] + ['ZCR_Generated', 'PitchMean_Generated', 'PitchStd_Generated', 'SpectralCentroid_Generated', 'SpectralBandwidth_Generated', 'SpectralRolloff_Generated', 'Harmonic_Generated', 'Percussive_Generated']\n",
    "features_compelling_avg = [f'MFCC_{i}_CompellingAvg' for i in range(13)] + ['ZCR_CompellingAvg', 'PitchMean_CompellingAvg', 'PitchStd_CompellingAvg', 'SpectralCentroid_CompellingAvg', 'SpectralBandwidth_CompellingAvg', 'SpectralRolloff_CompellingAvg', 'Harmonic_CompellingAvg', 'Percussive_CompellingAvg']\n",
    "\n",
    "# Normalize the feature values\n",
    "scaler = StandardScaler()\n",
    "generated_features_scaled = scaler.fit_transform(comparison_df[features_generated])\n",
    "compelling_avg_scaled = scaler.transform([avg_compelling_features])\n",
    "\n",
    "# Calculate Euclidean distances from each generated voice to the average compelling voice\n",
    "distances = np.sqrt(((generated_features_scaled - compelling_avg_scaled) ** 2).sum(axis=1))\n",
    "\n",
    "# Add distances to the comparison DataFrame\n",
    "comparison_df['Distance_to_Avg_Compelling'] = distances\n",
    "\n",
    "# Sort the DataFrame by distances (ascending order so that the smallest distances are at the top)\n",
    "comparison_df_sorted = comparison_df.sort_values('Distance_to_Avg_Compelling')\n",
    "\n",
    "# Print or save the entire sorted DataFrame\n",
    "print(comparison_df_sorted[['File_Generated', 'Distance_to_Avg_Compelling']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d74399",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
